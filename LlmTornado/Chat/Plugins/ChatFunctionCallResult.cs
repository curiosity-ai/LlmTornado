using System.Collections.Generic;
using LlmTornado.Code;

namespace LlmTornado.Chat.Plugins;

public class ChatFunctionCallResult
{
    public bool    Ok                           { get; private set; }
    public string? Error                        { get; private set; }
    public object? Result                       { get; private set; }
    public object? PostRenderData               { get; private set; }
    public bool    AllowSuccessiveFunctionCalls { get; private set; }

    /// <summary>
    /// Function call cancelled or failed. This is the most general constructor, use specialized overloads if applicable. Include a descriptive reason why, LLM uses this
    /// </summary>
    /// <param name="errorMessage"></param>
    public ChatFunctionCallResult(string errorMessage, ChatFunctionCallResultParameterErrors reason)
    {
        Error = errorMessage;
        SerializeError();
    }

    /// <summary>
    /// Function call cancelled. Include a descriptive reason why, LLM uses this
    /// </summary>
    /// <param name="paramErrorKind"></param>
    /// <param name="paramName"></param>
    public ChatFunctionCallResult(ChatFunctionCallResultParameterErrors paramErrorKind, string paramName)
    {
        Error = paramErrorKind switch
        {
            ChatFunctionCallResultParameterErrors.MissingRequiredParameter     => $"No function called. Missing required parameter '{paramName}'.",
            ChatFunctionCallResultParameterErrors.RequiredParameterInvalidType => $"No function called. Required parameter '{paramName}' has invalid type.",
            _                                                                  => Error
        };

        SerializeError();
    }

    /// <summary>
    /// Function call succeeded. LLM will use <see cref="data"/> to generate the next message<br/>
    /// Note this function automatically adds key "result": "ok" to the <see cref="data"/> if it's not found
    /// </summary>
    /// <param name="data">Dictionary / anonymous object / JSON serializable class</param>
    /// <param name="allowSuccessiveFunctionCalls">If true, LLM may decide to call this or any other available function again before passing control to the user</param>
    public ChatFunctionCallResult(object? data, bool allowSuccessiveFunctionCalls = true)
    {
        BaseCtor(data, allowSuccessiveFunctionCalls);
    }

    /// <summary>
    /// Function call succeeded. LLM will use <see cref="data"/> to generate the next message<br/>
    /// Note this function automatically adds key "result": "ok" to the <see cref="data"/> if it's not found
    /// </summary>
    /// <param name="llmFeedbackData">Dictionary / anonymous object / JSON serializable class</param>
    /// <param name="passtroughData">This data will be stored in the chat message and are available after the messages is fully streamed for rendering</param>
    /// <param name="allowSuccessiveFunctionCalls">If true, LLM may decide to call this or any other available function again before passing control to the user</param>
    public ChatFunctionCallResult(object? llmFeedbackData, object? passtroughData, bool allowSuccessiveFunctionCalls = true)
    {
        llmFeedbackData ??= new
        {
            result = "ok"
        };

        BaseCtor(llmFeedbackData, allowSuccessiveFunctionCalls);
        PostRenderData = passtroughData;
    }

    private void BaseCtor(object? data, bool allowSuccessiveFunctionCalls = true)
    {
        Result                       = data;
        Ok                           = true;
        AllowSuccessiveFunctionCalls = allowSuccessiveFunctionCalls;

        if (Result is not null)
        {
            Dictionary<string, object?>? dict = data?.ToDictionary();
            dict ??= [];
            dict.AddOrUpdate("result", "ok");
            Result = data;
        }
    }

    private void SerializeError()
    {
        Result = new
        {
            result  = "error",
            message = Error
        };
    }
}